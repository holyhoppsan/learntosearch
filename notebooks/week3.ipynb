{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0a - Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "should_use_small_data = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0 - Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sentencepiece as sp\n",
    "from datasets import list_datasets\n",
    "from datasets import load_dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2r/kktc65rd2cg5yxp_jj0bvxjw0000gn/T/ipykernel_91919/1468372510.py:1: FutureWarning: list_datasets is deprecated and will be removed in the next major version of datasets. Use 'huggingface_hub.list_datasets' instead.\n",
      "  datasets_list = list_datasets()\n",
      "Downloading readme: 100%|██████████| 9.48k/9.48k [00:00<00:00, 9.53MB/s]\n",
      "Downloading data: 100%|██████████| 21.4M/21.4M [00:04<00:00, 4.80MB/s]\n",
      "Downloading data: 100%|██████████| 175M/175M [00:18<00:00, 9.70MB/s] \n",
      "Downloading data: 100%|██████████| 20.5M/20.5M [00:03<00:00, 6.09MB/s]\n",
      "Generating validation split: 100%|██████████| 10047/10047 [00:00<00:00, 97313.10 examples/s]\n",
      "Generating train split: 100%|██████████| 82326/82326 [00:00<00:00, 159795.17 examples/s]\n",
      "Generating test split: 100%|██████████| 9650/9650 [00:00<00:00, 175111.23 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             answers  \\\n",
      "0  [Results-Based Accountability is a disciplined...   \n",
      "1                                              [Yes]   \n",
      "2                                    [20-25 minutes]   \n",
      "3                       [$11 to $22 per square foot]   \n",
      "4                      [Due to symptoms in the body]   \n",
      "\n",
      "                                            passages  \\\n",
      "0  {'is_selected': [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]...   \n",
      "1  {'is_selected': [0, 1, 0, 0, 0, 0, 0], 'passag...   \n",
      "2  {'is_selected': [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]...   \n",
      "3  {'is_selected': [0, 0, 0, 0, 0, 0, 0, 0, 1], '...   \n",
      "4  {'is_selected': [0, 0, 1, 0, 0, 0, 0, 0], 'pas...   \n",
      "\n",
      "                                               query  query_id   query_type  \\\n",
      "0                                        what is rba     19699  description   \n",
      "1                       was ronald reagan a democrat     19700  description   \n",
      "2  how long do you need for sydney and surroundin...     19701      numeric   \n",
      "3                    price to install tile in shower     19702      numeric   \n",
      "4                    why conversion observed in body     19703  description   \n",
      "\n",
      "  wellFormedAnswers  \n",
      "0                []  \n",
      "1                []  \n",
      "2                []  \n",
      "3                []  \n",
      "4                []  \n"
     ]
    }
   ],
   "source": [
    "datasets_list = list_datasets()\n",
    "ms_df_dict = load_dataset(\"ms_marco\", \"v1.1\")\n",
    "ms_train_df = ms_df_dict['train']\n",
    "ms_validation_df = ms_df_dict['validation']\n",
    "ms_test_df = ms_df_dict['test']\n",
    "\n",
    "# Convert to Pandas\n",
    "ms_train_df = pd.DataFrame(ms_train_df)\n",
    "ms_validation_df = pd.DataFrame(ms_validation_df)\n",
    "ms_test_df = pd.DataFrame(ms_test_df)\n",
    "\n",
    "print(ms_train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup a smaller dataset for debugging purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if should_use_small_data: \n",
    "    ms_train_df = ms_train_df.iloc[0:1000]\n",
    "    ms_validation_df = ms_validation_df.iloc[0:600]\n",
    "    ms_test_df = ms_test_df[0:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step X - Sentence piece preparation\n",
    "\n",
    "First we create the corpus from the combination from the combination of the the queries and the documents (both positive and negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_string = \"query\"\n",
    "\n",
    "# gather all of queries into one list\n",
    "all_queries = list(ms_train_df_short[\"query\"]) + list(ms_validation_df_short[\"query\"]) + list(ms_test_df_short[\"query\"])\n",
    "\n",
    "assert (len(all_queries) == len(ms_train_df_short) + len(ms_validation_df_short) + len(ms_test_df_short))\n",
    "\n",
    "# gather all documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = all_queries\n",
    "\n",
    "corpus_filename = \"./datasets/ms_marco_corpus.txt\"\n",
    "with open(corpus_filename, \"w\") as corpus_file:\n",
    "    for sentence in corpus:\n",
    "        corpus_file.write(sentence + os.linesep)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learntosearch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
